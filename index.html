<!DOCTYPE html>
<html lang="uk">
    <head>
        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Lumanosimo&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="css/style.css" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Lesson 3</title>
    </head>
    <body>
        <header>
            <nav id="top">
                <ul>
                    <li class="link_list">
                        <a class="a_1"
                            href="https://www.radiosvoboda.org/a/shtuchnyi-intelekt-zagrozy-i-mozhlyvisti/31145992.html"
                            >Посилання на статтю</a
                        >
                    </li>
                    <li class="link_list">
                        <a class="a_2"
                            href="https://www.radiosvoboda.org/a/shtuchnyi-intelekt-zagrozy-i-mozhlyvisti/31145992.html"
                            >Посилання на статтю</a
                        >
                    </li>
                    <li class="link_list">
                        <a class="a_3"
                            href="https://www.radiosvoboda.org/a/shtuchnyi-intelekt-zagrozy-i-mozhlyvisti/31145992.html"
                            >Посилання на статтю</a
                        >
                    </li>
                    <li class="link_list">
                        <a class="a_4"
                            href="https://www.radiosvoboda.org/a/shtuchnyi-intelekt-zagrozy-i-mozhlyvisti/31145992.html"
                            >Посилання на статтю</a
                        >
                    </li>
                </ul>
            </nav>
        </header>
        <main>
            <h1>Штучний інтелект і людина: загрози і можливості</h1>
            <p class="text_1">
                Засновник новаторських високотехнологічних компаній Tesla та SpaceX Ілон Маск
                ще у 2018 році під час конференції South by Southwest в Остіні (Техас, США)
                жбурнув у маси пророчо-ексцентричну тезу про те, що штучний інтелект (AI) є
                небезпечнішим за ядерну зброю. Чи це так?
            </p>

            <p class="text_2">
                Заява Ілона Маска, мов каталізатор, додала наснаги прихильникам концепції
                «всезагального» штучного інтелекту (AGI – artificial generic intelligence), що
                пророкують повстання універсального машинного інтелекту, який набуде
                здібностей, досі невідомих природному інтелекту людини, й відтак занапастить
                людство.
            </p>

            <p class="text_3">Реальність, однак, дещо інша.</p>
            <img
                src="img/51621B6A-8534-4B87-A0E2-126FFEECA08A_w650_r1_s.jpeg"
                alt="нема картинки"
            />
            <h2>Обмеження штучного інтелекту</h2>
            <p class="text_4">
                Ми спостерігаємо, що сучасні системи штучного інтелекту, попри фантастичні
                успіхи – як, наприклад, у царині шахів, де уже досить давно існують комп’ютерні
                програми, що доволі легко перемагають навіть найсильніших у світі шахістів, або
                ж те, що значна кількість великих компанію оперують чат ботами, що
                автоматизують та прискорюють обробку звернень їхніх клієнтів без залучення
                людей-операторів – мають істотні й фундаментальні обмеження.
            </p>

            <p class="text_5" >Ось лише деякі з них:</p>
            <ul>
                <li class="list_item_1">
                    системи штучного інтелекту успішно вирішують лише якийсь єдиний тип задач –
                    той, для якого ці системи було спроєктовано від самого початку;
                </li>
                <li class="list_item_2">
                    у них немає можливості «перемикати контекст», переключаючись з одного типу
                    задач на інший, так, як це вміють робити люди.
                </li>

                <li class="list_item_3">
                    для того, щоб зуміти виконувати свою задачу, системам штучного інтелекту
                    потрібен певний час для навчання, а також інформація щодо «еталонної
                    правди». Тобто необхідна певна кількість еталонних даних, на яких систему
                    навчають перш, ніж запустити в експлуатацію. І це стосується навіть
                    надсучасних нейронних мереж;
                </li>

                <li class="list_item_4">
                    у моменти істотних змін зовнішніх умов, коли вхідні параметри для системи
                    штучного інтелекту перестають поводити себе в такий спосіб, як це
                    спостерігалось у момент її початкового навчання, виникає необхідність
                    тимчасового зняття системи з експлуатації для повторного навчання вже за
                    нових умов.
                </li>
            </ul>
            <p class="text_6">
                Усі ці вищезгадані обмеження натякають на існування «деміургів» – тих, хто
                визначає цілі систем штучного інтелекту, організує їхнє навчання, формує для
                них навчальні дані (ту ж саму «еталонну правду»), визначає правила й соціальні
                норми вжитку систем штучного інтелекту та зрештою запускає їх в експлуатацію.
                Тобто мова іде про людей.
            </p>
            <img
                src="img/DC8949ED-E9F3-4E2A-8261-89023303CB46_w650_r1_s.jpeg"
                alt="нема картинки"
            />
            <h2>Ризики застосування</h2>
            <p class="text_7">
                Водночас, уже стало ясно, що ризики від безконтрольного чи неетичного
                застосування технологій штучного інтелекту становлять цілком реальні небезпеки
                для людства.
            </p>

            <p class="text_8">А саме:</p>
            <ul>
                <li class="list_item_5">
                    втрата робочих місць людьми через автоматизацію рутинних повторюваних
                    операцій;
                </li>
                <li class="list_item_6">порушення приватності, ледь не до повної її руйнації;</li>
                <li class="list_item_7">
                    Deepfakes «синтез слів «глибинне навчання» та «підробка» – методика синтезу
                    зображення чи аудіоряду мовлення людини, яка базується на штучному
                    інтелекті; вона використовується для поєднання і накладення наявних
                    зображень та відео на вихідні зображення або відеоролики».
                </li>
                <img src="img/BB544AD2-0756-46A4-9AD7-799315761F7C_w650_r1_s.jpeg" alt="" />
                <li class="list_item_8">
                    автоматизована зброя, що поцілюватиме живі об'єкти без втручання
                    людей-операторів;
                </li>
                <li class="list_item_9">
                    помилково-упереджені рішення систем через викривленість початкових
                    навчальних даних (algorithmic bias).
                </li>
                <p>
                    Яскравим прикладом подібної ситуації був
                    <a
                        target="_blank"
                        href="https://novyny.online.ua/algoritmi-youtube-zabanili-kanal-pro-shahi-cherez-rasizm_n830279/"
                        >бан</a
                    >
                    платформою YouTube каналу хорватського шахіста Антоніо Радича, одного з
                    популярних світових шахових оглядачів, через підозру в «расизмі» на
                    підставі аналізу матеріалів його каналу системами штучного інтелекту.
                </p>

                <li class="list_item_10">
                    консервування соціально-економічної нерівності між різними верствами
                    населення й націями в світі.
                </li>
            </ul>
            <p>
                Однак, знаючи про способи створення й навчання сучасних систем штучного
                інтелекту, неважко пересвідчитися в тому, що ці ризики можуть стати реальними
                не через власне самі системи, а через отих самих «втаємничених невідомих» –
                «деміургів». Тобто, знову ж таки, людей.
            </p>
            <img src="img/EC1BA4EB-F307-49B9-92E6-060599B1F417_w650_r1_s.jpeg" alt="" />
            <h2>Можливості</h2>
            <p>
                Власне, ми впритул наблизилися до точки біфуркації, після проходження якої ми
                (людство в цілому) можемо опинитися в одному з двох цілком протилежних станів.
            </p>
            <h3>Цивілізація може перетворитися у:</h3>
            <ul>
                <li>
                    суспільство тотального контролю (хтось міг би назвати його
                    ультраіндустріальним суспільством), в якому «деміурги», озброєні засобами
                    систем штучного інтелекту, отримають можливість на кожному регулювати життя
                    більшості людей.
                </li>

                <li>
                    суспільство творців, де системи штучного інтелекту допоможуть людям
                    звільнитися від рутини, відчути солідарність та спрямувати свій вільний час
                    на творчу діяльність.
                </li>
            </ul>
            <p>
                Про небезпеку першого сценарію попереджав ще Іван Єфремов – видатний
                енциклопедист, палеонтолог світового рівня, археолог, мандрівник, філософ та
                самобутній письменник-фантаст.
            </p>
            <p>
                У 1968 році було вперше опубліковано його пророчий роман «Година Бика». Хоч
                роман і був фантастичним за антуражем, зображені там суспільні формації двох
                планет були насправді варіантами розвитку майбутнього для нас – землян.
            </p>
            <img src="img/3B8FFDA2-7A9A-4B30-9CD2-385EE8B7E782_w650_r1_s.jpeg" alt="" />
            <p>
                В одному з тих світів (Торманс) ми спостерігаємо суспільство, де все ретельно
                регламентовано, і всі верстви населення підпорядковані чіткім приписам від
                «вищих» (їх автор називає «змієносцями») та «верховних» (Ради Чотирьох). А
                наглядають за виконанням приписів тамтешньої еліти автоматизовані засоби
                контролю, які можна було б із легкістю назвати системами штучного інтелекту.
            </p>
            <p>
                Спроби реалізації подібної ультраіндустріальної моделі помітні в сучасному
                Китаї, де система соціального рейтингу громадян (social credit system) під
                управлінням систем штучного інтелекту, створення якої було анонсовано ще шість
                років тому,
                <a
                    target="_blank"
                    href="https://algorithmwatch.org/en/chinas-social-credit-system-overdue/"
                    >була запущена в тестову експлуатацію</a
                >
                у декількох провінціях країни. Поки вони ще далекі від повномасштабного
                впровадження.
            </p>
            <p>
                У романі Івана Єфремова жителі Тормансу таки змогли вирватися із пастки
                «деміургів», відновивши соціальну солідарність та довіру один до одного.
            </p>
            <p>
                Але це знову не про системи штучного інтелекту, а про нас – людей і нашу
                здатність змінюватися на краще.
            </p>

            <p>
                Чи зможемо ми виправдати віру в людство видатного письменника-провидця? Чи нас
                поглине нас тотальний електронний ультра-індустріальний кінець цивілізації?
            </p>

            <p>Це стане зрозумілим вже найближчими роками.</p>

            <p>
                Георгій Вишня – управлінець у сфері IT, фахівець із систем штучного інтелекту,
                машинного навчання, есеїст.
            </p>
            <p>
                Думки, висловлені в рубриці «Точка зору», передають погляди самих авторів і не
                конче відображають позицію Радіо Свобода
            </p>
        </main>
        <footer>
            <a href="#top">Вверх</a>
        </footer>
    </body>
</html>
